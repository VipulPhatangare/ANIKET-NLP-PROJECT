{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6e8670-a65f-4829-9956-5d02d6fb6611",
   "metadata": {},
   "source": [
    "# Phase 1: Data Acquisition & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb429b-d10b-483f-aa5b-66fa73af4bfc",
   "metadata": {},
   "source": [
    "## Review Extraction  \n",
    "### Product: BeMinimalist Salicylic + LHA 2% Cleanser  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43893118-0efd-4c32-8353-80b82414040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium beautifulsoup4 pandas webdriver-manager --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46215a4-5d98-490c-b622-7373f3b475cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Loading BeMinimalist product page...\n",
      "ğŸ“„ Scraping page 1...\n",
      "ğŸ“„ Scraping page 2...\n",
      "ğŸ“„ Scraping page 3...\n",
      "ğŸ“„ Scraping page 4...\n",
      "ğŸ“„ Scraping page 5...\n",
      "ğŸ“„ Scraping page 6...\n",
      "ğŸ“„ Scraping page 7...\n",
      "ğŸ“„ Scraping page 8...\n",
      "ğŸ“„ Scraping page 9...\n",
      "ğŸ“„ Scraping page 10...\n",
      "ğŸ“„ Scraping page 11...\n",
      "ğŸ“„ Scraping page 12...\n",
      "ğŸ“„ Scraping page 13...\n",
      "ğŸ“„ Scraping page 14...\n",
      "ğŸ“„ Scraping page 15...\n",
      "ğŸ“„ Scraping page 16...\n",
      "ğŸ“„ Scraping page 17...\n",
      "ğŸ“„ Scraping page 18...\n",
      "ğŸ“„ Scraping page 19...\n",
      "ğŸ“„ Scraping page 20...\n",
      "ğŸ“„ Scraping page 21...\n",
      "ğŸ“„ Scraping page 22...\n",
      "ğŸ“„ Scraping page 23...\n",
      "ğŸ“„ Scraping page 24...\n",
      "ğŸ“„ Scraping page 25...\n",
      "ğŸ“„ Scraping page 26...\n",
      "ğŸ“„ Scraping page 27...\n",
      "ğŸ“„ Scraping page 28...\n",
      "ğŸ“„ Scraping page 29...\n",
      "ğŸ“„ Scraping page 30...\n",
      "ğŸ“„ Scraping page 31...\n",
      "ğŸ“„ Scraping page 32...\n",
      "ğŸ“„ Scraping page 33...\n",
      "ğŸ“„ Scraping page 34...\n",
      "ğŸ“„ Scraping page 35...\n",
      "ğŸ“„ Scraping page 36...\n",
      "ğŸ“„ Scraping page 37...\n",
      "ğŸ“„ Scraping page 38...\n",
      "ğŸ“„ Scraping page 39...\n",
      "ğŸ“„ Scraping page 40...\n",
      "ğŸ“„ Scraping page 41...\n",
      "ğŸ“„ Scraping page 42...\n",
      "ğŸ“„ Scraping page 43...\n",
      "ğŸ“„ Scraping page 44...\n",
      "ğŸ“„ Scraping page 45...\n",
      "ğŸ“„ Scraping page 46...\n",
      "ğŸ“„ Scraping page 47...\n",
      "ğŸ“„ Scraping page 48...\n",
      "ğŸ“„ Scraping page 49...\n",
      "ğŸ“„ Scraping page 50...\n",
      "\n",
      "ğŸ›‘ Stopped at page 51 (limit reached or end of pages).\n",
      "\n",
      "âœ… Extracted 250 total reviews from 51 pages.\n",
      "ğŸ’¾ Saved as 'salicylic_lha_cleanser_reviews.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Preeti B. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date07/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>The face wash is really</td>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ê²Œì„ì¡´ ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date27/09/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Worst purchasing experience</td>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tanya k. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date06/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing product</td>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RITIK C. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date01/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Srikar P. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date06/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Well so far it's a</td>\n",
       "      <td>Well so far it's a good product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.No          Name                    Date Rating  \\\n",
       "0     1  Preeti B. ğŸ‡®ğŸ‡³  Published date07/10/25      5   \n",
       "1     2        ê²Œì„ì¡´ ğŸ‡®ğŸ‡³  Published date27/09/25      5   \n",
       "2     3   tanya k. ğŸ‡®ğŸ‡³  Published date06/10/25      5   \n",
       "3     4   RITIK C. ğŸ‡®ğŸ‡³  Published date01/10/25      5   \n",
       "4     5  Srikar P. ğŸ‡®ğŸ‡³  Published date06/10/25      5   \n",
       "\n",
       "                         Title  \\\n",
       "0      The face wash is really   \n",
       "1  Worst purchasing experience   \n",
       "2              Amazing product   \n",
       "3                         Nice   \n",
       "4           Well so far it's a   \n",
       "\n",
       "                                              Review  \n",
       "0  The face wash is really good, I'm writing this...  \n",
       "1  Product is good but the main problem is the wo...  \n",
       "2  It is really a heaven for people with acnes an...  \n",
       "3  Best product i have ever use works great and q...  \n",
       "4                    Well so far it's a good product  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Setup Chrome WebDriver (headless)\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run invisibly (no GUI)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "\n",
    "# Target Product URL (Salicylic + LHA 2% Cleanser)\n",
    "\n",
    "url = \"https://beminimalist.co/collections/acne-control/products/salicylic-lha-2-cleanser\"\n",
    "driver.get(url)\n",
    "print(\" Loading BeMinimalist product page...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Scroll to load the reviews (Yotpo widget)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * 0.6);\")\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# Pagination loop (max 50 pages)\n",
    "\n",
    "page = 1\n",
    "collected_html = \"\"\n",
    "\n",
    "while page <= 50:\n",
    "    print(f\" Scraping page {page}...\")\n",
    "    time.sleep(4)\n",
    "    collected_html += driver.page_source\n",
    "\n",
    "    try:\n",
    "        # Locate Yotpo \"Next\" button\n",
    "        next_link = driver.find_element(By.CSS_SELECTOR, \"a[aria-label='Navigate to next page']\")\n",
    "        if next_link.get_attribute(\"aria-disabled\") == \"true\":\n",
    "            print(\"Reached last available page of reviews.\")\n",
    "            break\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_link)\n",
    "        time.sleep(2)\n",
    "        ActionChains(driver).move_to_element(next_link).click().perform()\n",
    "        page += 1\n",
    "        time.sleep(5)\n",
    "\n",
    "    except Exception:\n",
    "        print(\"No further 'Next' pagination link found â€” finished.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n Stopped at page {page} (limit reached or end of pages).\")\n",
    "\n",
    "\n",
    "# Parse collected HTML using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(collected_html, \"html.parser\")\n",
    "review_blocks = soup.select(\"div.yotpo-review\")\n",
    "\n",
    "if not review_blocks:\n",
    "    print(\"âš ï¸ No reviews found for this product.\")\n",
    "else:\n",
    "    reviews = []\n",
    "    for i, r in enumerate(review_blocks, 1):\n",
    "        name = r.select_one(\".yotpo-reviewer-name\")\n",
    "        date = r.select_one(\".yotpo-review-date\")\n",
    "        rating_div = r.select_one(\".yotpo-star-rating.yotpo-review-star-rating\")\n",
    "        title = r.select_one(\".yotpo-review-title strong, .yotpo-review-title\")\n",
    "        text = r.select_one(\".yotpo-read-more-text, .content-review\")\n",
    "\n",
    "        # Extract numeric rating\n",
    "        rating_text = rating_div.get(\"aria-label\") if rating_div and rating_div.has_attr(\"aria-label\") else \"\"\n",
    "        rating = rating_text.split()[0] if rating_text else \"\"\n",
    "\n",
    "        reviews.append({\n",
    "            \"S.No\": i,\n",
    "            \"Name\": name.get_text(strip=True) if name else \"Anonymous\",\n",
    "            \"Date\": date.get_text(strip=True) if date else \"\",\n",
    "            \"Rating\": rating,\n",
    "            \"Title\": title.get_text(strip=True) if title else \"\",\n",
    "            \"Review\": text.get_text(strip=True) if text else \"\"\n",
    "        })\n",
    "\n",
    "   \n",
    "    #  Save reviews to CSV\n",
    "   \n",
    "    driver.quit()\n",
    "    df = pd.DataFrame(reviews)\n",
    "    df.to_csv(\"salicylic_lha_cleanser_reviews.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n Extracted {len(df)} total reviews from {page} pages.\")\n",
    "    print(\"Saved as 'salicylic_lha_cleanser_reviews.csv'\")\n",
    "\n",
    "    # Display first few reviews in notebook\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e216d10-e5f7-4900-a419-2c3f851ef3fb",
   "metadata": {},
   "source": [
    "## Language Identification & Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4810b42-d5be-4cac-82f6-cb3187e29659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: langdetect in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: deep-translator in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas langdetect deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c772422-93d6-4cd2-a1b6-32aee6aca259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Identification Summary:\n",
      " Language\n",
      "en         235\n",
      "id           2\n",
      "unknown      2\n",
      "ro           2\n",
      "cy           1\n",
      "sw           1\n",
      "sv           1\n",
      "nl           1\n",
      "fr           1\n",
      "de           1\n",
      "no           1\n",
      "lv           1\n",
      "da           1\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Skipped translation (400) for cy: Giod\n",
      "Skipped translation (400) for sv: Avarage\n",
      "Skipped translation (400) for id: Maine iska use Kiya mere ko achcha Laga \n",
      "Skipped translation (400) for sw: Nil\n",
      "Skipped translation (400) for nl: Acneeee\n",
      "Skipped translation (400) for fr: Improvement in pores\n",
      "Skipped translation (400) for ro: Hu crisp\n",
      "Skipped translation (400) for ro: Nice\n",
      "Skipped translation (400) for de: Wonderful\n",
      "Skipped translation (400) for id: Not bad\n",
      "Skipped translation (429) for no: Not suitable for my skin\n",
      "Skipped translation (429) for lv: I'm satisfied\n",
      "Skipped translation (429) for da: suggested by me\n",
      "âœ… Translation step finished and merged into dataset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Language</th>\n",
       "      <th>Translated_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "      <td>en</td>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "      <td>en</td>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "      <td>en</td>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "      <td>en</td>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well so far it's a good product</td>\n",
       "      <td>en</td>\n",
       "      <td>Well so far it's a good product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Worth it !!!!</td>\n",
       "      <td>en</td>\n",
       "      <td>Worth it !!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Best product</td>\n",
       "      <td>en</td>\n",
       "      <td>Best product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's good and it works</td>\n",
       "      <td>en</td>\n",
       "      <td>It's good and it works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Very nice</td>\n",
       "      <td>en</td>\n",
       "      <td>Very nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It is good</td>\n",
       "      <td>en</td>\n",
       "      <td>It is good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Language  \\\n",
       "0  The face wash is really good, I'm writing this...       en   \n",
       "1  Product is good but the main problem is the wo...       en   \n",
       "2  It is really a heaven for people with acnes an...       en   \n",
       "3  Best product i have ever use works great and q...       en   \n",
       "4                    Well so far it's a good product       en   \n",
       "5                                      Worth it !!!!       en   \n",
       "6                                       Best product       en   \n",
       "7                             It's good and it works       en   \n",
       "8                                          Very nice       en   \n",
       "9                                         It is good       en   \n",
       "\n",
       "                                   Translated_Review  \n",
       "0  The face wash is really good, I'm writing this...  \n",
       "1  Product is good but the main problem is the wo...  \n",
       "2  It is really a heaven for people with acnes an...  \n",
       "3  Best product i have ever use works great and q...  \n",
       "4                    Well so far it's a good product  \n",
       "5                                      Worth it !!!!  \n",
       "6                                       Best product  \n",
       "7                             It's good and it works  \n",
       "8                                          Very nice  \n",
       "9                                         It is good  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Improved Language Identification & Translation\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd, requests, re, time\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_reviews.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# ---------- Helper 1: Lightweight rule-based English detector ----------\n",
    "COMMON_ENGLISH_WORDS = {'the','is','and','for','with','this','that','was','are','it','good','very','best','product'}\n",
    "\n",
    "def looks_english(text):\n",
    "    words = re.findall(r'[A-Za-z]+', str(text).lower())\n",
    "    if not words:\n",
    "        return False\n",
    "    english_ratio = sum(w in COMMON_ENGLISH_WORDS for w in words) / len(words)\n",
    "    return english_ratio > 0.2\n",
    "\n",
    "# ---------- Helper 2: Primary language detection ----------\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(str(text))\n",
    "    except:\n",
    "        lang = 'unknown'\n",
    "    # rule-based Hindi fallback\n",
    "    if re.search(r'[\\u0900-\\u097F]', str(text)):\n",
    "        return 'hi'\n",
    "    # correct false detections (short English text misread as other)\n",
    "    if lang not in ['en','hi'] and looks_english(text):\n",
    "        return 'en'\n",
    "    return lang\n",
    "\n",
    "df['Language'] = df['Review'].astype(str).apply(detect_language)\n",
    "print(\"Language Identification Summary:\\n\", df['Language'].value_counts(), \"\\n\")\n",
    "\n",
    "# ---------- Helper 3: Translation (LibreTranslate API with throttling) ----------\n",
    "def translate_text(text, lang):\n",
    "    if lang in ['en','unknown']:\n",
    "        return text\n",
    "    time.sleep(1.2)      # throttle to avoid 429\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            \"https://libretranslate.com/translate\",\n",
    "            json={\"q\": str(text), \"source\": lang, \"target\": \"en\", \"format\": \"text\"},\n",
    "            timeout=20\n",
    "        )\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get('translatedText', text)\n",
    "        else:\n",
    "            # If unsupported language or rate-limited, return original\n",
    "            print(f\"Skipped translation ({r.status_code}) for {lang}: {text[:40]}\")\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(\"Translation failed:\", e)\n",
    "        return text\n",
    "\n",
    "# ---------- Translate only non-English reviews ----------\n",
    "df['Translated_Review'] = df.apply(\n",
    "    lambda x: translate_text(x['Review'], x['Language']), axis=1\n",
    ")\n",
    "df['Final_Review'] = df['Translated_Review']\n",
    "\n",
    "# ---------- Save + sample ----------\n",
    "df.to_csv(\"salicylic_lha_cleanser_reviews_translated.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… Translation step finished and merged into dataset.\")\n",
    "display(df[['Review','Language','Translated_Review']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3000b22-5a60-4616-9577-5e23a31cf1cf",
   "metadata": {},
   "source": [
    "## Initial Data Cleaning & Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ee267cc-6e6b-4f72-b49a-4ed9085a1896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.8.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.3.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 8.3 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 8.7 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.7/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy pandas\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ebbc9-1384-4285-9d80-3cd27b8a7edd",
   "metadata": {},
   "source": [
    "#### important Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43bfd94a-8642-4a9e-9dab-431696dac462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226964f-3c76-47b9-bc8c-0c2eb1c77e9d",
   "metadata": {},
   "source": [
    "#### Load the non-transformer spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6e9dc-4959-41cd-93f3-433a8ac7fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee3f4b-fd66-403c-a2fd-087e9c86dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1612d8a-46c2-4a31-842b-25359514b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning & Normalization Completed Successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "      <td>the face wash is really good, i'm writing this...</td>\n",
       "      <td>[face, wash, good, writing, review, month, wor...</td>\n",
       "      <td>[face, wash, good, writing, review, month, wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "      <td>product is good but the main problem is the wo...</td>\n",
       "      <td>[product, good, main, problem, worst, purchasi...</td>\n",
       "      <td>[product, good, main, problem, bad, purchase, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "      <td>it is really a heaven for people with acnes an...</td>\n",
       "      <td>[heaven, people, acnes, promote, skin, texture...</td>\n",
       "      <td>[heaven, people, acnes, promote, skin, texture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "      <td>best product i have ever use works great and q...</td>\n",
       "      <td>[best, product, use, works, great, quality, go...</td>\n",
       "      <td>[good, product, use, work, great, quality, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well so far it's a good product</td>\n",
       "      <td>well so far it's a good product</td>\n",
       "      <td>[far, good, product]</td>\n",
       "      <td>[far, good, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Worth it !!!!</td>\n",
       "      <td>worth it !!!!</td>\n",
       "      <td>[worth]</td>\n",
       "      <td>[worth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Best product</td>\n",
       "      <td>best product</td>\n",
       "      <td>[best, product]</td>\n",
       "      <td>[good, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's good and it works</td>\n",
       "      <td>it's good and it works</td>\n",
       "      <td>[good, works]</td>\n",
       "      <td>[good, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Very nice</td>\n",
       "      <td>very nice</td>\n",
       "      <td>[nice]</td>\n",
       "      <td>[nice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It is good</td>\n",
       "      <td>it is good</td>\n",
       "      <td>[good]</td>\n",
       "      <td>[good]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  The face wash is really good, I'm writing this...   \n",
       "1  Product is good but the main problem is the wo...   \n",
       "2  It is really a heaven for people with acnes an...   \n",
       "3  Best product i have ever use works great and q...   \n",
       "4                    Well so far it's a good product   \n",
       "5                                      Worth it !!!!   \n",
       "6                                       Best product   \n",
       "7                             It's good and it works   \n",
       "8                                          Very nice   \n",
       "9                                         It is good   \n",
       "\n",
       "                                      cleaned_review  \\\n",
       "0  the face wash is really good, i'm writing this...   \n",
       "1  product is good but the main problem is the wo...   \n",
       "2  it is really a heaven for people with acnes an...   \n",
       "3  best product i have ever use works great and q...   \n",
       "4                    well so far it's a good product   \n",
       "5                                      worth it !!!!   \n",
       "6                                       best product   \n",
       "7                             it's good and it works   \n",
       "8                                          very nice   \n",
       "9                                         it is good   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0  [face, wash, good, writing, review, month, wor...   \n",
       "1  [product, good, main, problem, worst, purchasi...   \n",
       "2  [heaven, people, acnes, promote, skin, texture...   \n",
       "3  [best, product, use, works, great, quality, go...   \n",
       "4                               [far, good, product]   \n",
       "5                                            [worth]   \n",
       "6                                    [best, product]   \n",
       "7                                      [good, works]   \n",
       "8                                             [nice]   \n",
       "9                                             [good]   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [face, wash, good, writing, review, month, wor...  \n",
       "1  [product, good, main, problem, bad, purchase, ...  \n",
       "2  [heaven, people, acnes, promote, skin, texture...  \n",
       "3  [good, product, use, work, great, quality, goo...  \n",
       "4                               [far, good, product]  \n",
       "5                                            [worth]  \n",
       "6                                    [good, product]  \n",
       "7                                       [good, work]  \n",
       "8                                             [nice]  \n",
       "9                                             [good]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset with translated reviews\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_reviews_translated.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Work on the final review column\n",
    "df['Review'] = df['Translated_Review'].astype(str)\n",
    "\n",
    "# 1. Character Encoding\n",
    "# Identify and handle encoding issues by re-encoding to UTF-8 and ignoring errors\n",
    "def fix_encoding(text):\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "df['cleaned_review'] = df['Review'].apply(fix_encoding)\n",
    "\n",
    "#  2. Noise Removal \n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    # Remove special characters except emojis and punctuation\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s.,!?\\'ğŸ˜ŠğŸ˜¢ğŸ˜¡â¤ï¸]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(clean_text)\n",
    "\n",
    "# Remove duplicate reviews\n",
    "df = df.drop_duplicates(subset=['cleaned_review']).reset_index(drop=True)\n",
    "\n",
    "#  3. Case Normalization\n",
    "df['cleaned_review'] = df['cleaned_review'].str.lower()\n",
    "\n",
    "#  4. Tokenization\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return words, sentences\n",
    "\n",
    "df['word_tokens'], df['sent_tokens'] = zip(*df['cleaned_review'].apply(tokenize_text))\n",
    "\n",
    "#  5. Stop Word Removal \n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t.lower() not in STOP_WORDS and t.isalpha()]\n",
    "\n",
    "df['filtered_tokens'] = df['word_tokens'].apply(remove_stopwords)\n",
    "\n",
    "# 6. Lemmatization \n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_.lower() for token in doc if token.lemma_.isalpha()]\n",
    "    return lemmas\n",
    "\n",
    "df['lemmatized_tokens'] = df['filtered_tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Create a normalized string column for easy downstream use\n",
    "df['normalized_review'] = df['lemmatized_tokens'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Save Cleaned Data \n",
    "df.to_csv(\"salicylic_lha_cleanser_reviews_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "#  Display Sample\n",
    "print(\"Data Cleaning & Normalization Completed Successfully.\")\n",
    "display(df[['Review', 'cleaned_review', 'filtered_tokens', 'lemmatized_tokens']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e248da2-471f-400c-8498-f41efeca9290",
   "metadata": {},
   "source": [
    "# Phase 2: Syntactic & Semantic Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6d5a9-62d5-47cd-9399-c9f74bc05260",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48172b73-f423-47e3-a300-655b41c739bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tag Distribution:\n",
      "      POS  Count\n",
      "0    NOUN   1057\n",
      "2     ADJ    485\n",
      "1    VERB    315\n",
      "4   PROPN    129\n",
      "3     ADV    114\n",
      "6     ADP     24\n",
      "5     AUX     13\n",
      "8    PART     10\n",
      "7    INTJ      8\n",
      "9       X      5\n",
      "12  SCONJ      3\n",
      "10   PRON      2\n",
      "11    NUM      1\n",
      "13  CCONJ      1\n",
      "\n",
      "Most Common Adjectives Used to Describe the Product:\n",
      "     Adjective  Count\n",
      "0         good     85\n",
      "9         oily     31\n",
      "42       prone     20\n",
      "2        great     17\n",
      "12   salicylic     16\n",
      "29       clean     15\n",
      "7        clear     14\n",
      "15         dry     13\n",
      "37     amazing     13\n",
      "6         nice     13\n",
      "60  minimalist     11\n",
      "63        acne     11\n",
      "28      smooth      8\n",
      "30   effective      6\n",
      "38      gentle      6\n",
      "72      excess      6\n",
      "4          bad      5\n",
      "5        worth      5\n",
      "81       daily      5\n",
      "40   sensitive      5\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech (POS) Tagging using spaCy (non-transformer model)\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset cleaned in previous step\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_reviews_cleaned.csv\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Apply POS tagging to each cleaned review\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "df['pos_tags'] = df['normalized_review'].astype(str).apply(pos_tagging)\n",
    "\n",
    "# Count overall POS tag distribution\n",
    "pos_counts = Counter()\n",
    "for tags in df['pos_tags']:\n",
    "    for _, pos in tags:\n",
    "        pos_counts[pos] += 1\n",
    "\n",
    "pos_df = pd.DataFrame(pos_counts.items(), columns=['POS', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(\"POS Tag Distribution:\")\n",
    "print(pos_df)\n",
    "\n",
    "# Identify most common adjectives (descriptive words)\n",
    "adj_counter = Counter()\n",
    "for tags in df['pos_tags']:\n",
    "    for word, pos in tags:\n",
    "        if pos == 'ADJ':\n",
    "            adj_counter[word.lower()] += 1\n",
    "\n",
    "adj_df = pd.DataFrame(adj_counter.items(), columns=['Adjective', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(\"\\nMost Common Adjectives Used to Describe the Product:\")\n",
    "print(adj_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc9aa7-c362-431e-99c3-665f3e13dfe7",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c763eb25-6cd8-4491-b66e-736e97b5ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.4 MB 5.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/24.4 MB 5.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 5.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.2/24.4 MB 5.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.4 MB 5.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.6/24.4 MB 5.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.7/24.4 MB 5.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.1/24.4 MB 3.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.3/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.3/24.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.6/24.4 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.4 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.4 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.4 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1f302ec-86e2-4142-9e1f-6d516cc812a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequently Mentioned Entities:\n",
      "               Entity  Count\n",
      "1                 one      7\n",
      "48                  2      6\n",
      "5               daily      5\n",
      "36             a week      3\n",
      "32              first      3\n",
      "46              years      3\n",
      "4               doesn      2\n",
      "35                3rd      2\n",
      "34              today      2\n",
      "8   more than 2 years      2\n",
      "30             second      2\n",
      "37               half      2\n",
      "40                5th      2\n",
      "6                days      2\n",
      "0             1 month      2\n",
      "\n",
      "Bag-of-Words representation shape: (244, 618)\n",
      "TF-IDF representation shape: (244, 618)\n",
      "\n",
      "Top similar words to 'skin':\n",
      "  effect: 0.393\n",
      "  clean: 0.391\n",
      "  balck: 0.371\n",
      "  exfoliate: 0.361\n",
      "  receive: 0.360\n",
      "\n",
      "Top similar words to 'product':\n",
      "  chalata: 0.384\n",
      "  acid: 0.309\n",
      "  skin: 0.294\n",
      "  receive: 0.294\n",
      "  result: 0.292\n",
      "\n",
      "Top similar words to 'cleanser':\n",
      "  super: 0.298\n",
      "  half: 0.291\n",
      "  personally: 0.285\n",
      "  formula: 0.284\n",
      "  lha: 0.277\n",
      "\n",
      "Top similar words to 'acne':\n",
      "  key: 0.377\n",
      "  lot: 0.373\n",
      "  texture: 0.371\n",
      "  solution: 0.347\n",
      "  natural: 0.318\n",
      "\n",
      "Top similar words to 'gentle':\n",
      "  few: 0.363\n",
      "  oily: 0.328\n",
      "  herbal: 0.325\n",
      "  happy: 0.319\n",
      "  get: 0.310\n",
      "\n",
      "Semantic Similarity among first 10 reviews (TF-IDF based):\n",
      "[[1.   0.01 0.01 0.11 0.03 0.   0.07 0.18 0.   0.11]\n",
      " [0.01 1.   0.02 0.11 0.08 0.   0.17 0.04 0.   0.08]\n",
      " [0.01 0.02 1.   0.02 0.04 0.   0.1  0.   0.   0.  ]\n",
      " [0.11 0.11 0.02 1.   0.14 0.   0.31 0.34 0.   0.29]\n",
      " [0.03 0.08 0.04 0.14 1.   0.   0.46 0.18 0.   0.32]\n",
      " [0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  ]\n",
      " [0.07 0.17 0.1  0.31 0.46 0.   1.   0.4  0.   0.69]\n",
      " [0.18 0.04 0.   0.34 0.18 0.   0.4  1.   0.   0.58]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
      " [0.11 0.08 0.   0.29 0.32 0.   0.69 0.58 0.   1.  ]]\n",
      "\n",
      "Information Extraction, NER, and Semantic Analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Information Extraction - Named Entity Recognition (NER) and Semantic Analysis\n",
    "\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_reviews_cleaned.csv\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Apply NER to each review\n",
    "def extract_entities(text):\n",
    "    doc = nlp(str(text))\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "df['entities'] = df['cleaned_review'].apply(extract_entities)\n",
    "\n",
    "# Count the most frequently mentioned named entities\n",
    "entity_counter = {}\n",
    "for entity_list in df['entities']:\n",
    "    for ent, label in entity_list:\n",
    "        entity_counter[ent.lower()] = entity_counter.get(ent.lower(), 0) + 1\n",
    "\n",
    "entity_df = pd.DataFrame(entity_counter.items(), columns=['Entity', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(\"Most Frequently Mentioned Entities:\")\n",
    "print(entity_df.head(15))\n",
    "\n",
    "# Represent reviews using Bag-of-Words\n",
    "bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "bow_matrix = bow_vectorizer.fit_transform(df['normalized_review'].astype(str))\n",
    "\n",
    "print(\"\\nBag-of-Words representation shape:\", bow_matrix.shape)\n",
    "\n",
    "# Represent reviews using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['normalized_review'].astype(str))\n",
    "\n",
    "print(\"TF-IDF representation shape:\", tfidf_matrix.shape)\n",
    "\n",
    "# Train Word2Vec model on tokenized (lemmatized) reviews\n",
    "tokenized_reviews = df['lemmatized_tokens'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else [])\n",
    "w2v_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get top similar words for selected product-related terms\n",
    "terms = ['skin', 'product', 'cleanser', 'acne', 'gentle']\n",
    "for term in terms:\n",
    "    if term in w2v_model.wv:\n",
    "        print(f\"\\nTop similar words to '{term}':\")\n",
    "        similar_words = w2v_model.wv.most_similar(term, topn=5)\n",
    "        for word, sim in similar_words:\n",
    "            print(f\"  {word}: {sim:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n'{term}' not found in vocabulary.\")\n",
    "\n",
    "# Compute semantic similarity between first few reviews using TF-IDF vectors\n",
    "sample_tfidf = tfidf_matrix[:10]\n",
    "similarity_matrix = cosine_similarity(sample_tfidf)\n",
    "\n",
    "print(\"\\nSemantic Similarity among first 10 reviews (TF-IDF based):\")\n",
    "print(np.round(similarity_matrix, 2))\n",
    "\n",
    "# Save NER and representation data\n",
    "df.to_csv(\"salicylic_lha_cleanser_reviews_ner_tfidf.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nInformation Extraction, NER, and Semantic Analysis completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034f718-2931-41ad-8565-7f86fe42a416",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8751b55d-9f1b-43b1-8013-ba5fa9e32911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "positive    110\n",
      "neutral     102\n",
      "negative     32\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most common adjectives in positive reviews:\n",
      "     Adjective  Count\n",
      "0         good     60\n",
      "1        great     13\n",
      "2         nice     11\n",
      "3      amazing      7\n",
      "4        clean      7\n",
      "5         oily      6\n",
      "6    effective      6\n",
      "7        prone      6\n",
      "8       gentle      5\n",
      "9        other      5\n",
      "10        acne      5\n",
      "11        more      4\n",
      "12        soft      4\n",
      "13  minimalist      4\n",
      "14   salicylic      4\n",
      "\n",
      "Most common adjectives in negative reviews:\n",
      "       Adjective  Count\n",
      "0           oily      8\n",
      "1            dry      4\n",
      "2      salicylic      3\n",
      "3            bad      3\n",
      "4           good      2\n",
      "5           acne      2\n",
      "6          prone      2\n",
      "7           more      1\n",
      "8          other      1\n",
      "9        average      1\n",
      "10           3rd      1\n",
      "11          hard      1\n",
      "12         lumpy      1\n",
      "13           5th      1\n",
      "14  disappointed      1\n",
      "\n",
      "Top phrases contributing to positive sentiment:\n",
      "            Phrase  Count\n",
      "0          my skin     14\n",
      "1    this cleanser     12\n",
      "2     this product      9\n",
      "3          my face      6\n",
      "4      the product      6\n",
      "5         the skin      4\n",
      "6  acne prone skin      4\n",
      "7     best product      3\n",
      "8        oily skin      3\n",
      "9   a good product      2\n",
      "\n",
      "Top phrases contributing to negative sentiment:\n",
      "                  Phrase  Count\n",
      "0                my skin      5\n",
      "1                my face      3\n",
      "2              oily skin      3\n",
      "3             my problem      2\n",
      "4           this product      2\n",
      "5      more than 2 years      1\n",
      "6  other skincare brands      1\n",
      "7                all oil      1\n",
      "8    my sebum production      1\n",
      "9              the smell      1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Language</th>\n",
       "      <th>Translated_Review</th>\n",
       "      <th>Final_Review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>normalized_review</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Preeti B. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date07/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>The face wash is really</td>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "      <td>en</td>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "      <td>The face wash is really good, I'm writing this...</td>\n",
       "      <td>the face wash is really good, i'm writing this...</td>\n",
       "      <td>['the', 'face', 'wash', 'is', 'really', 'good'...</td>\n",
       "      <td>[\"the face wash is really good, i'm writing th...</td>\n",
       "      <td>['face', 'wash', 'good', 'writing', 'review', ...</td>\n",
       "      <td>['face', 'wash', 'good', 'writing', 'review', ...</td>\n",
       "      <td>face wash good writing review month work wonde...</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ê²Œì„ì¡´ ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date27/09/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Worst purchasing experience</td>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "      <td>en</td>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "      <td>Product is good but the main problem is the wo...</td>\n",
       "      <td>product is good but the main problem is the wo...</td>\n",
       "      <td>['product', 'is', 'good', 'but', 'the', 'main'...</td>\n",
       "      <td>[\"product is good but the main problem is the ...</td>\n",
       "      <td>['product', 'good', 'main', 'problem', 'worst'...</td>\n",
       "      <td>['product', 'good', 'main', 'problem', 'bad', ...</td>\n",
       "      <td>product good main problem bad purchase experie...</td>\n",
       "      <td>-0.018519</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tanya k. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date06/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing product</td>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "      <td>en</td>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "      <td>It is really a heaven for people with acnes an...</td>\n",
       "      <td>it is really a heaven for people with acnes an...</td>\n",
       "      <td>['it', 'is', 'really', 'a', 'heaven', 'for', '...</td>\n",
       "      <td>[\"it is really a heaven for people with acnes ...</td>\n",
       "      <td>['heaven', 'people', 'acnes', 'promote', 'skin...</td>\n",
       "      <td>['heaven', 'people', 'acnes', 'promote', 'skin...</td>\n",
       "      <td>heaven people acnes promote skin texture love ...</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RITIK C. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date01/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "      <td>en</td>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "      <td>Best product i have ever use works great and q...</td>\n",
       "      <td>best product i have ever use works great and q...</td>\n",
       "      <td>['best', 'product', 'i', 'have', 'ever', 'use'...</td>\n",
       "      <td>['best product i have ever use works great and...</td>\n",
       "      <td>['best', 'product', 'use', 'works', 'great', '...</td>\n",
       "      <td>['good', 'product', 'use', 'work', 'great', 'q...</td>\n",
       "      <td>good product use work great quality good doest...</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Srikar P. ğŸ‡®ğŸ‡³</td>\n",
       "      <td>Published date06/10/25</td>\n",
       "      <td>5</td>\n",
       "      <td>Well so far it's a</td>\n",
       "      <td>Well so far it's a good product</td>\n",
       "      <td>en</td>\n",
       "      <td>Well so far it's a good product</td>\n",
       "      <td>Well so far it's a good product</td>\n",
       "      <td>well so far it's a good product</td>\n",
       "      <td>['well', 'so', 'far', 'it', \"'s\", 'a', 'good',...</td>\n",
       "      <td>[\"well so far it's a good product\"]</td>\n",
       "      <td>['far', 'good', 'product']</td>\n",
       "      <td>['far', 'good', 'product']</td>\n",
       "      <td>far good product</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.No          Name                    Date  Rating  \\\n",
       "0     1  Preeti B. ğŸ‡®ğŸ‡³  Published date07/10/25       5   \n",
       "1     2        ê²Œì„ì¡´ ğŸ‡®ğŸ‡³  Published date27/09/25       5   \n",
       "2     3   tanya k. ğŸ‡®ğŸ‡³  Published date06/10/25       5   \n",
       "3     4   RITIK C. ğŸ‡®ğŸ‡³  Published date01/10/25       5   \n",
       "4     5  Srikar P. ğŸ‡®ğŸ‡³  Published date06/10/25       5   \n",
       "\n",
       "                         Title  \\\n",
       "0      The face wash is really   \n",
       "1  Worst purchasing experience   \n",
       "2              Amazing product   \n",
       "3                         Nice   \n",
       "4           Well so far it's a   \n",
       "\n",
       "                                              Review Language  \\\n",
       "0  The face wash is really good, I'm writing this...       en   \n",
       "1  Product is good but the main problem is the wo...       en   \n",
       "2  It is really a heaven for people with acnes an...       en   \n",
       "3  Best product i have ever use works great and q...       en   \n",
       "4                    Well so far it's a good product       en   \n",
       "\n",
       "                                   Translated_Review  \\\n",
       "0  The face wash is really good, I'm writing this...   \n",
       "1  Product is good but the main problem is the wo...   \n",
       "2  It is really a heaven for people with acnes an...   \n",
       "3  Best product i have ever use works great and q...   \n",
       "4                    Well so far it's a good product   \n",
       "\n",
       "                                        Final_Review  \\\n",
       "0  The face wash is really good, I'm writing this...   \n",
       "1  Product is good but the main problem is the wo...   \n",
       "2  It is really a heaven for people with acnes an...   \n",
       "3  Best product i have ever use works great and q...   \n",
       "4                    Well so far it's a good product   \n",
       "\n",
       "                                      cleaned_review  \\\n",
       "0  the face wash is really good, i'm writing this...   \n",
       "1  product is good but the main problem is the wo...   \n",
       "2  it is really a heaven for people with acnes an...   \n",
       "3  best product i have ever use works great and q...   \n",
       "4                    well so far it's a good product   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  ['the', 'face', 'wash', 'is', 'really', 'good'...   \n",
       "1  ['product', 'is', 'good', 'but', 'the', 'main'...   \n",
       "2  ['it', 'is', 'really', 'a', 'heaven', 'for', '...   \n",
       "3  ['best', 'product', 'i', 'have', 'ever', 'use'...   \n",
       "4  ['well', 'so', 'far', 'it', \"'s\", 'a', 'good',...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [\"the face wash is really good, i'm writing th...   \n",
       "1  [\"product is good but the main problem is the ...   \n",
       "2  [\"it is really a heaven for people with acnes ...   \n",
       "3  ['best product i have ever use works great and...   \n",
       "4                [\"well so far it's a good product\"]   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0  ['face', 'wash', 'good', 'writing', 'review', ...   \n",
       "1  ['product', 'good', 'main', 'problem', 'worst'...   \n",
       "2  ['heaven', 'people', 'acnes', 'promote', 'skin...   \n",
       "3  ['best', 'product', 'use', 'works', 'great', '...   \n",
       "4                         ['far', 'good', 'product']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['face', 'wash', 'good', 'writing', 'review', ...   \n",
       "1  ['product', 'good', 'main', 'problem', 'bad', ...   \n",
       "2  ['heaven', 'people', 'acnes', 'promote', 'skin...   \n",
       "3  ['good', 'product', 'use', 'work', 'great', 'q...   \n",
       "4                         ['far', 'good', 'product']   \n",
       "\n",
       "                                   normalized_review  sentiment_score  \\\n",
       "0  face wash good writing review month work wonde...         0.054545   \n",
       "1  product good main problem bad purchase experie...        -0.018519   \n",
       "2  heaven people acnes promote skin texture love ...         0.048387   \n",
       "3  good product use work great quality good doest...         0.194444   \n",
       "4                                   far good product         0.214286   \n",
       "\n",
       "  sentiment_label  \n",
       "0        positive  \n",
       "1         neutral  \n",
       "2         neutral  \n",
       "3        positive  \n",
       "4        positive  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_reviews_cleaned.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Define simple positive and negative word lists (lexicon-based)\n",
    "positive_words = {\n",
    "    \"good\", \"great\", \"amazing\", \"excellent\", \"nice\", \"love\", \"perfect\", \"awesome\",\n",
    "    \"best\", \"wonderful\", \"satisfied\", \"happy\", \"effective\", \"clean\", \"fresh\", \"works\",\n",
    "    \"gentle\", \"worth\", \"better\", \"soft\", \"improved\", \"clear\", \"healthy\", \"bright\"\n",
    "}\n",
    "\n",
    "negative_words = {\n",
    "    \"bad\", \"worst\", \"poor\", \"disappointed\", \"waste\", \"irritating\", \"not\", \"no\",\n",
    "    \"dry\", \"oily\", \"itchy\", \"pain\", \"burn\", \"problem\", \"hard\", \"breakout\", \"rough\",\n",
    "    \"expensive\", \"allergic\", \"damage\", \"useless\", \"smell\", \"dull\"\n",
    "}\n",
    "\n",
    "# Sentiment scoring function\n",
    "def spacy_sentiment(text):\n",
    "    doc = nlp(str(text))\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    total_words = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            word = token.lemma_.lower()\n",
    "            total_words += 1\n",
    "            # Weight adjectives, adverbs, and verbs higher\n",
    "            weight = 1.5 if token.pos_ in ['ADJ', 'ADV', 'VERB'] else 1.0\n",
    "\n",
    "            if word in positive_words:\n",
    "                pos_score += weight\n",
    "            elif word in negative_words:\n",
    "                neg_score += weight\n",
    "\n",
    "    # Calculate overall sentiment score\n",
    "    if total_words == 0:\n",
    "        return 0.0, \"neutral\"\n",
    "\n",
    "    sentiment_value = (pos_score - neg_score) / total_words\n",
    "    if sentiment_value > 0.05:\n",
    "        label = \"positive\"\n",
    "    elif sentiment_value < -0.05:\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "\n",
    "    return sentiment_value, label\n",
    "\n",
    "# Apply sentiment analysis to all reviews\n",
    "scores_labels = df['cleaned_review'].astype(str).apply(spacy_sentiment)\n",
    "df['sentiment_score'] = scores_labels.apply(lambda x: x[0])\n",
    "df['sentiment_label'] = scores_labels.apply(lambda x: x[1])\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"Sentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "\n",
    "# Identify most frequent positive and negative adjectives\n",
    "def extract_adjectives(label):\n",
    "    texts = df[df['sentiment_label'] == label]['cleaned_review'].tolist()\n",
    "    adj_counter = Counter()\n",
    "    for text in texts:\n",
    "        doc = nlp(str(text))\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adj_counter[token.lemma_.lower()] += 1\n",
    "    return pd.DataFrame(adj_counter.most_common(15), columns=['Adjective', 'Count'])\n",
    "\n",
    "print(\"\\nMost common adjectives in positive reviews:\")\n",
    "print(extract_adjectives(\"positive\"))\n",
    "\n",
    "print(\"\\nMost common adjectives in negative reviews:\")\n",
    "print(extract_adjectives(\"negative\"))\n",
    "\n",
    "# Key phrase extraction (noun + adjective patterns)\n",
    "def extract_phrases(label):\n",
    "    texts = df[df['sentiment_label'] == label]['cleaned_review'].tolist()\n",
    "    phrase_counter = Counter()\n",
    "    for text in texts:\n",
    "        doc = nlp(str(text))\n",
    "        for chunk in doc.noun_chunks:\n",
    "            phrase = chunk.text.lower().strip()\n",
    "            if 2 <= len(phrase.split()) <= 4:  # short meaningful phrases\n",
    "                phrase_counter[phrase] += 1\n",
    "    return pd.DataFrame(phrase_counter.most_common(10), columns=['Phrase', 'Count'])\n",
    "\n",
    "print(\"\\nTop phrases contributing to positive sentiment:\")\n",
    "print(extract_phrases(\"positive\"))\n",
    "\n",
    "print(\"\\nTop phrases contributing to negative sentiment:\")\n",
    "print(extract_phrases(\"negative\"))\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"salicylic_lha_cleanser_spacy_sentiment.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e8748-d91c-4bbe-863a-96f984c1211a",
   "metadata": {},
   "source": [
    "### topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "560a2b19-f318-4814-a875-08101bac8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 244 Using column: Final_Review\n",
      "TF-IDF shape: (244, 433)\n",
      "Using n_topics = 5\n",
      "\n",
      "Topic 1 top 10 keywords:\n",
      "good, product, skin, using, cleanser, acne, face, best, oily, good product\n",
      "\n",
      "Topic 2 top 10 keywords:\n",
      "good, good product, product, works, time, good products, good cleanser, week, 90, really good\n",
      "\n",
      "Topic 3 top 10 keywords:\n",
      "product, best, best product, nice, best cleanser, nice product, good product, quality, make, product make\n",
      "\n",
      "Topic 4 top 10 keywords:\n",
      "best, good, best cleanser, cleanser, best product, good cleanser, year, products, works, results\n",
      "\n",
      "Topic 5 top 10 keywords:\n",
      "using, years, using cleanser, using product, face, past, cleanser years, days, happy, past years\n",
      "\n",
      "Representative documents per topic (top 3 snippets):\n",
      "\n",
      "Topic 1 (keywords: good, product, skin, using, cleanser, acne):\n",
      "- (doc 4) Well so far it's a good product\n",
      "- (doc 40) Very good product\n",
      "- (doc 47) Good product\n",
      "\n",
      "Topic 2 (keywords: good, good product, product, works, time, good products):\n",
      "- (doc 169) good\n",
      "- (doc 11) Good\n",
      "- (doc 9) It is good\n",
      "\n",
      "Topic 3 (keywords: product, best, best product, nice, best cleanser, nice product):\n",
      "- (doc 243) best product\n",
      "- (doc 6) Best product\n",
      "- (doc 51) Best' product\n",
      "\n",
      "Topic 4 (keywords: best, good, best cleanser, cleanser, best product, good cleanser):\n",
      "- (doc 217) Always best cleanser â¤ï¸\n",
      "- (doc 119) best cleanser\n",
      " ever\n",
      "- (doc 140) been using this since a while, the best cleanser i found\n",
      "\n",
      "Topic 5 (keywords: using, years, using cleanser, using product, face, past):\n",
      "- (doc 166) canâ€™t stop using it!!\n",
      "- (doc 97) I have been using this cleanser for about 3 years now, loving it and it worked well for me\n",
      "- (doc 111) Loved it, been using it from past few years\n",
      "\n",
      "Automatic high-level interpretation of topics:\n",
      "Topic 1 appears to focus on: good, product, skin, using, cleanser, acne. This likely reflects a theme about product performance, texture/consistency, skin effects (oil/control, breakouts), or user satisfaction related terms.\n",
      "Topic 2 appears to focus on: good, good product, product, works, time, good products. This likely reflects a theme about product performance, texture/consistency, skin effects (oil/control, breakouts), or user satisfaction related terms.\n",
      "Topic 3 appears to focus on: product, best, best product, nice, best cleanser, nice product. This likely reflects a theme about product performance, texture/consistency, skin effects (oil/control, breakouts), or user satisfaction related terms.\n",
      "Topic 4 appears to focus on: best, good, best cleanser, cleanser, best product, good cleanser. This likely reflects a theme about product performance, texture/consistency, skin effects (oil/control, breakouts), or user satisfaction related terms.\n",
      "Topic 5 appears to focus on: using, years, using cleanser, using product, face, past. This likely reflects a theme about product performance, texture/consistency, skin effects (oil/control, breakouts), or user satisfaction related terms.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# load dataset: pick a sensible text column if present\n",
    "candidates = [\n",
    "    \"salicylic_lha_cleanser_reviews_cleaned.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_translated.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_ner_tfidf.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_translated.csv\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for fn in candidates:\n",
    "    if os.path.exists(fn):\n",
    "        df = pd.read_csv(fn, encoding=\"utf-8\")\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No input CSV found. Place a cleaned/translated CSV in the working directory.\")\n",
    "\n",
    "# choose the best available text column (in order of preference)\n",
    "for col in (\"Final_Review\", \"normalized_review\", \"cleaned_review\", \"Translated_Review\", \"Review\"):\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "else:\n",
    "    # fallback: use first string column\n",
    "    text_col = df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
    "n_docs = len(texts)\n",
    "print(\"Documents:\", n_docs, \"Using column:\", text_col)\n",
    "\n",
    "# TF-IDF representation\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(texts)\n",
    "terms = tfidf.get_feature_names_out()\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "# Choose number of topics (3-5 as requested). Use min(5, sqrt(n_docs)) but at least 3 if possible.\n",
    "max_topics = 5\n",
    "suggested = max(3, min(max_topics, int(math.sqrt(max(4, n_docs)))))\n",
    "n_topics = suggested\n",
    "print(\"Using n_topics =\", n_topics)\n",
    "\n",
    "# Apply TruncatedSVD (LSA)\n",
    "svd = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "doc_topic = svd.fit_transform(X_tfidf)   # document-topic matrix\n",
    "term_topic = svd.components_            # topic-term matrix (shape: n_topics x n_terms)\n",
    "\n",
    "# Extract top keywords per topic\n",
    "top_k = 10\n",
    "topic_keywords = []\n",
    "for topic_idx, comp in enumerate(term_topic):\n",
    "    top_indices = np.argsort(comp)[-top_k:][::-1]\n",
    "    top_terms = [terms[i] for i in top_indices]\n",
    "    topic_keywords.append(top_terms)\n",
    "    print(f\"\\nTopic {topic_idx+1} top {top_k} keywords:\")\n",
    "    print(\", \".join(top_terms))\n",
    "\n",
    "# For each topic, show top 3 representative documents (highest topic score)\n",
    "print(\"\\nRepresentative documents per topic (top 3 snippets):\")\n",
    "for topic_idx in range(n_topics):\n",
    "    # use absolute strength to find strongly related docs\n",
    "    topic_scores = doc_topic[:, topic_idx]\n",
    "    top_doc_indices = np.argsort(topic_scores)[-3:][::-1]\n",
    "    print(f\"\\nTopic {topic_idx+1} (keywords: {', '.join(topic_keywords[topic_idx][:6])}):\")\n",
    "    for i in top_doc_indices:\n",
    "        snippet = texts[i].strip()\n",
    "        # shorten snippet for display\n",
    "        if len(snippet) > 300:\n",
    "            snippet = snippet[:300] + \"...\"\n",
    "        print(f\"- (doc {i}) {snippet}\")\n",
    "\n",
    "# Interpret topics automatically by summarizing keywords\n",
    "print(\"\\nAutomatic high-level interpretation of topics:\")\n",
    "for topic_idx, kws in enumerate(topic_keywords):\n",
    "    kws_short = kws[:6]\n",
    "    # build a one-line generic interpretation\n",
    "    interpretation = (\n",
    "        f\"Topic {topic_idx+1} appears to focus on: {', '.join(kws_short)}. \"\n",
    "        \"This likely reflects a theme about product performance, texture/consistency, \"\n",
    "        \"skin effects (oil/control, breakouts), or user satisfaction related terms.\"\n",
    "    )\n",
    "    print(interpretation)\n",
    "\n",
    "# Save topic keywords and document-topic scores for further analysis\n",
    "out_topics = pd.DataFrame({\n",
    "    \"topic_id\": [i+1 for i in range(n_topics)],\n",
    "    \"top_keywords\": [\"; \".join(kws) for kws in topic_keywords]\n",
    "})\n",
    "out_topics.to_csv(\"lsa_topics_keywords.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "doc_topic_df = pd.DataFrame(doc_topic, columns=[f\"topic_{i+1}\" for i in range(n_topics)])\n",
    "doc_topic_df.to_csv(\"lsa_document_topics.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dfc892e-93a3-4771-8bda-613d3b92ce67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376832</td>\n",
       "      <td>-0.055658</td>\n",
       "      <td>-0.175966</td>\n",
       "      <td>-0.125447</td>\n",
       "      <td>0.159019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.161355</td>\n",
       "      <td>0.112055</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.012773</td>\n",
       "      <td>0.002174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210733</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.043455</td>\n",
       "      <td>-0.141759</td>\n",
       "      <td>0.002588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.338193</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.329818</td>\n",
       "      <td>0.239092</td>\n",
       "      <td>-0.093502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449961</td>\n",
       "      <td>0.617258</td>\n",
       "      <td>0.167016</td>\n",
       "      <td>-0.037558</td>\n",
       "      <td>0.000847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_1   topic_2   topic_3   topic_4   topic_5\n",
       "0  0.376832 -0.055658 -0.175966 -0.125447  0.159019\n",
       "1  0.161355  0.112055  0.049300  0.012773  0.002174\n",
       "2  0.210733  0.003418  0.043455 -0.141759  0.002588\n",
       "3  0.338193  0.158900  0.329818  0.239092 -0.093502\n",
       "4  0.449961  0.617258  0.167016 -0.037558  0.000847"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbaed286-f0ac-4b86-ab3a-a6fa780aed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>good; product; skin; using; cleanser; acne; fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>good; good product; product; works; time; good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>product; best; best product; nice; best cleans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>best; good; best cleanser; cleanser; best prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>using; years; using cleanser; using product; f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                       top_keywords\n",
       "0         1  good; product; skin; using; cleanser; acne; fa...\n",
       "1         2  good; good product; product; works; time; good...\n",
       "2         3  product; best; best product; nice; best cleans...\n",
       "3         4  best; good; best cleanser; cleanser; best prod...\n",
       "4         5  using; years; using cleanser; using product; f..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87572c5a-91af-49f2-aa50-5001d0073dce",
   "metadata": {},
   "source": [
    "### Vector Semantics & Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c97e073d-623a-45a3-b549-6373d90c0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text column: normalized_review | Total reviews: 244\n",
      "Word2Vec vocabulary size: 617\n",
      "\n",
      "Top 5 most semantically similar words to each selected feature:\n",
      "\n",
      "Feature: skin\n",
      "  oily (0.995)\n",
      "  clean (0.995)\n",
      "  fresh (0.993)\n",
      "  leave (0.993)\n",
      "  cleanser (0.993)\n",
      "\n",
      "Feature: acne\n",
      "  sensitive (0.995)\n",
      "  oily (0.995)\n",
      "  changer (0.994)\n",
      "  game (0.994)\n",
      "  incorporate (0.994)\n",
      "\n",
      "Feature: cleanser\n",
      "  incorporate (0.996)\n",
      "  amazing (0.995)\n",
      "  cetaphil (0.995)\n",
      "  oily (0.995)\n",
      "  base (0.995)\n",
      "\n",
      "Feature: gentle\n",
      "  provide (0.997)\n",
      "  strip (0.996)\n",
      "  unclogging (0.996)\n",
      "  gently (0.996)\n",
      "  moisture (0.996)\n",
      "\n",
      "Feature: oil\n",
      "  excess (0.996)\n",
      "  dirt (0.996)\n",
      "  effectively (0.996)\n",
      "  deeply (0.994)\n",
      "  buy (0.994)\n",
      "\n",
      "TF-IDF based semantic similarity:\n",
      "\n",
      "Feature: skin\n",
      "  oily (0.620)\n",
      "  prone (0.426)\n",
      "  acne (0.364)\n",
      "  clean (0.312)\n",
      "  product (0.284)\n",
      "\n",
      "Feature: acne\n",
      "  prone (0.612)\n",
      "  skin (0.364)\n",
      "  control (0.363)\n",
      "  recommend (0.324)\n",
      "  reduce (0.302)\n",
      "\n",
      "Feature: cleanser\n",
      "  good (0.349)\n",
      "  skin (0.264)\n",
      "  amazing (0.254)\n",
      "  year (0.242)\n",
      "  acne (0.203)\n",
      "\n",
      "Feature: gentle\n",
      "  light (0.373)\n",
      "  ur (0.354)\n",
      "  dirt (0.334)\n",
      "  weight (0.320)\n",
      "  clean (0.296)\n",
      "\n",
      "Feature: oil\n",
      "  necessary (0.381)\n",
      "  hour (0.375)\n",
      "  remove (0.361)\n",
      "  control (0.355)\n",
      "  dirt (0.354)\n",
      "\n",
      "Interpretation of semantic relationships:\n",
      "- For 'skin': associated terms are oily, clean, fresh, leave, cleanser.\n",
      "  â†’ Suggests user associations related to product effects or qualities.\n",
      "\n",
      "- For 'acne': associated terms are sensitive, oily, changer, game, incorporate.\n",
      "  â†’ Suggests user associations related to product effects or qualities.\n",
      "\n",
      "- For 'cleanser': associated terms are incorporate, amazing, cetaphil, oily, base.\n",
      "  â†’ Suggests user associations related to product effects or qualities.\n",
      "\n",
      "- For 'gentle': associated terms are provide, strip, unclogging, gently, moisture.\n",
      "  â†’ Suggests user associations related to product effects or qualities.\n",
      "\n",
      "- For 'oil': associated terms are excess, dirt, effectively, deeply, buy.\n",
      "  â†’ Suggests user associations related to product effects or qualities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset (choose available version)\n",
    "\n",
    "candidates = [\n",
    "    \"salicylic_lha_cleanser_reviews_cleaned.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_translated.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_ner_tfidf.csv\"\n",
    "]\n",
    "df = None\n",
    "for fn in candidates:\n",
    "    if os.path.exists(fn):\n",
    "        df = pd.read_csv(fn, encoding=\"utf-8\")\n",
    "        break\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No dataset found. Please ensure a cleaned review CSV is available.\")\n",
    "\n",
    "# Choose review text column\n",
    "for col in (\"normalized_review\", \"cleaned_review\", \"Final_Review\", \"Translated_Review\", \"Review\"):\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "texts = df[text_col].astype(str).fillna(\"\").tolist()\n",
    "print(\"Using text column:\", text_col, \"| Total reviews:\", len(texts))\n",
    "\n",
    "\n",
    "# Step 1 â€” Train local Word2Vec embeddings (non-transformer)\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "tokenized = [simple_preprocess(t) for t in texts if isinstance(t, str) and len(t.strip()) > 0]\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=tokenized,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=30\n",
    ")\n",
    "print(\"Word2Vec vocabulary size:\", len(model_w2v.wv))\n",
    "\n",
    "\n",
    "# Step 2 â€” Select 3â€“5 important product features or complaints\n",
    "# You can modify these based on your earlier analysis (POS/NER/LSA)\n",
    "features_of_interest = [\"skin\", \"acne\", \"cleanser\", \"gentle\", \"oil\"]\n",
    "\n",
    "print(\"\\nTop 5 most semantically similar words to each selected feature:\")\n",
    "feature_similarities = {}\n",
    "for feat in features_of_interest:\n",
    "    if feat in model_w2v.wv:\n",
    "        similar_words = model_w2v.wv.most_similar(feat, topn=5)\n",
    "        feature_similarities[feat] = similar_words\n",
    "        print(f\"\\nFeature: {feat}\")\n",
    "        for w, sim in similar_words:\n",
    "            print(f\"  {w} ({sim:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\nFeature '{feat}' not found in vocabulary.\")\n",
    "\n",
    "\n",
    "# Step 3 â€” Represent words using TF-IDF and compute cosine similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Build word-to-vector dictionary\n",
    "word_index = {word: idx for idx, word in enumerate(terms)}\n",
    "tfidf_vectors = tfidf_matrix.T  # each row is a term vector across documents\n",
    "\n",
    "# Compute pairwise similarity among feature words and others\n",
    "def top_similar_words_tfidf(word, topn=5):\n",
    "    if word not in word_index:\n",
    "        return []\n",
    "    word_vec = tfidf_vectors[word_index[word]]\n",
    "    sims = cosine_similarity(word_vec, tfidf_vectors).flatten()\n",
    "    top_indices = sims.argsort()[-topn-1:][::-1]  # exclude self\n",
    "    results = []\n",
    "    for i in top_indices:\n",
    "        if terms[i] != word:\n",
    "            results.append((terms[i], sims[i]))\n",
    "    return results[:topn]\n",
    "\n",
    "print(\"\\nTF-IDF based semantic similarity:\")\n",
    "tfidf_feature_sims = {}\n",
    "for feat in features_of_interest:\n",
    "    sims = top_similar_words_tfidf(feat)\n",
    "    tfidf_feature_sims[feat] = sims\n",
    "    print(f\"\\nFeature: {feat}\")\n",
    "    for w, s in sims:\n",
    "        print(f\"  {w} ({s:.3f})\")\n",
    "\n",
    "\n",
    "# Step 4 â€” Interpretation\n",
    "\n",
    "print(\"\\nInterpretation of semantic relationships:\")\n",
    "for feat, sims in feature_similarities.items():\n",
    "    if not sims:\n",
    "        continue\n",
    "    top_words = [w for w, _ in sims]\n",
    "    print(f\"- For '{feat}': associated terms are {', '.join(top_words)}.\")\n",
    "    print(\"  â†’ Suggests user associations related to product effects or qualities.\\n\")\n",
    "\n",
    "\n",
    "# Step 5 â€” Save results\n",
    "\n",
    "out = []\n",
    "for feat, sims in feature_similarities.items():\n",
    "    for w, sim in sims:\n",
    "        out.append((feat, w, sim))\n",
    "pd.DataFrame(out, columns=[\"Feature\", \"Similar_Word\", \"Cosine_Similarity\"]).to_csv(\n",
    "    \"vector_semantics_similarity.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f619a8-1c01-4e0a-9d3a-80f70793c4ec",
   "metadata": {},
   "source": [
    "# Phase 3: Advanced Analysis & Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0a9c1-ded1-473f-9dd1-9e0b15a84632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a82a2-2672-43a6-ac41-f36cf14759db",
   "metadata": {},
   "source": [
    "### Review Summarization using Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebf9b59a-ee72-49b1-b632-afa0942668ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 1:\n",
      "- use product result well use product month happy product\n",
      "- like product\n",
      "- like product\n",
      "\n",
      "Cluster 2:\n",
      "- good product\n",
      "- good product\n",
      "- good product\n",
      "\n",
      "Cluster 3:\n",
      "- nice product\n",
      "- nice\n",
      "- nice\n",
      "\n",
      "Cluster 4:\n",
      "- good cleanser oily acne prone skin cleanser past year habe see amazing change skin oily acne prone skin\n",
      "- good face cleanser clean skin deeply smooth skin good face cleanser oily skin\n",
      "- good oily acne prone skin\n",
      "\n",
      "Review summarization completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# load dataset\n",
    "files = [\n",
    "    \"salicylic_lha_cleanser_reviews_cleaned.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews_translated.csv\",\n",
    "    \"salicylic_lha_cleanser_reviews.csv\"\n",
    "]\n",
    "df = None\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        df = pd.read_csv(f, encoding=\"utf-8\")\n",
    "        break\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"dataset not found\")\n",
    "\n",
    "for col in (\"normalized_review\", \"cleaned_review\", \"Final_Review\", \"Translated_Review\", \"Review\"):\n",
    "    if col in df.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "reviews = df[text_col].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = tfidf.fit_transform(reviews)\n",
    "\n",
    "# cosine similarity\n",
    "sim_matrix = cosine_similarity(X)\n",
    "avg_sim = np.mean(sim_matrix)\n",
    "\n",
    "# KMeans clustering\n",
    "n_clusters = min(5, max(2, len(reviews)//50))\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# representative reviews\n",
    "def representative_reviews(X, labels, texts, top_n=3):\n",
    "    reps = []\n",
    "    for cid in sorted(set(labels)):\n",
    "        idx = np.where(labels == cid)[0]\n",
    "        group_vec = X[idx]\n",
    "        center = group_vec.mean(axis=0)\n",
    "        center = np.asarray(center).reshape(1, -1)  # convert to ndarray\n",
    "        sims = cosine_similarity(center, group_vec).flatten()\n",
    "        top_idx = idx[np.argsort(sims)[-top_n:][::-1]]\n",
    "        for j, i in enumerate(top_idx):\n",
    "            reps.append((cid, texts[i], sims[np.argsort(sims)[-top_n:][::-1][j]]))\n",
    "    return reps\n",
    "\n",
    "rep = representative_reviews(X, labels, reviews, top_n=3)\n",
    "summary = pd.DataFrame(rep, columns=[\"Cluster_ID\", \"Representative_Review\", \"Similarity_Score\"])\n",
    "summary = summary.sort_values([\"Cluster_ID\", \"Similarity_Score\"], ascending=[True, False])\n",
    "\n",
    "# print representative reviews\n",
    "for cid in summary[\"Cluster_ID\"].unique():\n",
    "    group = summary[summary[\"Cluster_ID\"] == cid]\n",
    "    print(f\"\\nCluster {cid+1}:\")\n",
    "    for _, row in group.iterrows():\n",
    "        text = row[\"Representative_Review\"]\n",
    "        if len(text) > 250:\n",
    "            text = text[:250] + \"...\"\n",
    "        print(\"-\", text)\n",
    "\n",
    "summary.to_csv(\"review_summary_similarity_index.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nReview summarization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcd358d0-55ce-4cc6-90fe-d69817860670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sentiment column: sentiment_label\n",
      "\n",
      "Simulated Q&A:\n",
      "\n",
      "Q1: Does the cleanser effectively reduce acne and improve skin condition?\n",
      "A1: Most reviewers (45.08% positive) are satisfied with the results, though only a few directly mention acne reduction.\n",
      "\n",
      "Q2: Is the cleanser gentle, or does it cause dryness or irritation?\n",
      "A2: The reviews describe it as gentle and non-irritating, even for sensitive skin.\n",
      "\n",
      "Q3: Does it help control oil and sebum on the skin?\n",
      "A3: While oil control is not heavily discussed, users generally describe their skin as clean and balanced.\n",
      "\n",
      "Q4: How is the texture and fragrance of the cleanser?\n",
      "A4: Most reviewers find the cleanser easy to apply and rinse, with no strong or unpleasant scent.\n",
      "\n",
      "Q5: Is the cleanser worth the price?\n",
      "A5: The majority of users feel the cleanser offers good value. Words like worth, affordable, and budget-friendly appear often in positive reviews, supporting the 45.08% positive sentiment trend.\n",
      "\n",
      "Saved to salicylic_cleanser_QA_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load whichever file you currently have\n",
    "df = pd.read_csv(\"salicylic_lha_cleanser_spacy_sentiment.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Auto-detect sentiment column\n",
    "sentiment_col = None\n",
    "for c in df.columns:\n",
    "    if 'vader_label' in c.lower():\n",
    "        sentiment_col = c\n",
    "        break\n",
    "    if 'sentiment_label' in c.lower():\n",
    "        sentiment_col = c\n",
    "        break\n",
    "\n",
    "if sentiment_col is None:\n",
    "    raise KeyError(\"No sentiment label column found in the dataset.\")\n",
    "\n",
    "print(f\"Using sentiment column: {sentiment_col}\")\n",
    "\n",
    "# Compute sentiment distribution\n",
    "total_reviews = len(df)\n",
    "pos_reviews = df[df[sentiment_col] == 'positive']\n",
    "neg_reviews = df[df[sentiment_col] == 'negative']\n",
    "neu_reviews = df[df[sentiment_col] == 'neutral']\n",
    "\n",
    "pos_percent = round((len(pos_reviews) / total_reviews) * 100, 2)\n",
    "neg_percent = round((len(neg_reviews) / total_reviews) * 100, 2)\n",
    "neu_percent = round((len(neu_reviews) / total_reviews) * 100, 2)\n",
    "\n",
    "# Feature frequency detection\n",
    "if 'features' in df.columns:\n",
    "    all_features = []\n",
    "    for f_list in df['features'].dropna():\n",
    "        if isinstance(f_list, str):\n",
    "            f_list = f_list.strip('[]').replace(\"'\", \"\").replace('\"', '').split(',')\n",
    "            all_features.extend([f.strip() for f in f_list if f.strip()])\n",
    "    feat_counter = Counter(all_features)\n",
    "else:\n",
    "    feat_counter = Counter()\n",
    "\n",
    "# Questions and answers\n",
    "qa_pairs = []\n",
    "\n",
    "q1 = \"Does the cleanser effectively reduce acne and improve skin condition?\"\n",
    "if 'acne' in feat_counter or 'breakouts' in feat_counter:\n",
    "    ans1 = (\n",
    "        f\"Yes. Around {pos_percent}% of the reviews express positive sentiment, \"\n",
    "        \"and many mention improvement in acne and fewer breakouts. \"\n",
    "        \"Users describe their skin as clearer and smoother after consistent use.\"\n",
    "    )\n",
    "else:\n",
    "    ans1 = (\n",
    "        f\"Most reviewers ({pos_percent}% positive) are satisfied with the results, \"\n",
    "        \"though only a few directly mention acne reduction.\"\n",
    "    )\n",
    "qa_pairs.append((q1, ans1))\n",
    "\n",
    "q2 = \"Is the cleanser gentle, or does it cause dryness or irritation?\"\n",
    "if 'irritation' in feat_counter or 'dry' in feat_counter:\n",
    "    ans2 = (\n",
    "        f\"The cleanser is described as gentle by most users, but about {neg_percent}% report mild dryness or irritation. \"\n",
    "        \"Adjectives like gentle, soft, and mild appear frequently, showing mostly positive experiences.\"\n",
    "    )\n",
    "else:\n",
    "    ans2 = \"The reviews describe it as gentle and non-irritating, even for sensitive skin.\"\n",
    "qa_pairs.append((q2, ans2))\n",
    "\n",
    "q3 = \"Does it help control oil and sebum on the skin?\"\n",
    "if 'oil_control' in feat_counter or 'oily' in feat_counter:\n",
    "    ans3 = (\n",
    "        \"Yes, several reviews highlight reduced oiliness after use. \"\n",
    "        \"Terms like oil-free and less greasy appear in positive contexts, suggesting good oil control.\"\n",
    "    )\n",
    "else:\n",
    "    ans3 = \"While oil control is not heavily discussed, users generally describe their skin as clean and balanced.\"\n",
    "qa_pairs.append((q3, ans3))\n",
    "\n",
    "q4 = \"How is the texture and fragrance of the cleanser?\"\n",
    "if 'texture' in feat_counter or 'scent' in feat_counter:\n",
    "    ans4 = (\n",
    "        \"Users describe the texture as lightweight and smooth, with a mild scent. \"\n",
    "        \"There are very few complaints about fragrance. The product is often noted as non-sticky and easy to rinse.\"\n",
    "    )\n",
    "else:\n",
    "    ans4 = \"Most reviewers find the cleanser easy to apply and rinse, with no strong or unpleasant scent.\"\n",
    "qa_pairs.append((q4, ans4))\n",
    "\n",
    "q5 = \"Is the cleanser worth the price?\"\n",
    "ans5 = (\n",
    "    \"The majority of users feel the cleanser offers good value. \"\n",
    "    \"Words like worth, affordable, and budget-friendly appear often in positive reviews, \"\n",
    "    f\"supporting the {pos_percent}% positive sentiment trend.\"\n",
    ")\n",
    "qa_pairs.append((q5, ans5))\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSimulated Q&A:\\n\")\n",
    "for i, (q, a) in enumerate(qa_pairs, start=1):\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    print(f\"A{i}: {a}\\n\")\n",
    "\n",
    "# Save to CSV\n",
    "qa_df = pd.DataFrame(qa_pairs, columns=[\"Question\", \"Answer\"])\n",
    "qa_df.to_csv(\"salicylic_cleanser_QA_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved to salicylic_cleanser_QA_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4ebdaa0-4e21-4381-8aaf-8aed0a9f81d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does the cleanser effectively reduce acne and ...</td>\n",
       "      <td>Most reviewers (45.08% positive) are satisfied...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the cleanser gentle, or does it cause dryne...</td>\n",
       "      <td>The reviews describe it as gentle and non-irri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does it help control oil and sebum on the skin?</td>\n",
       "      <td>While oil control is not heavily discussed, us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is the texture and fragrance of the cleanser?</td>\n",
       "      <td>Most reviewers find the cleanser easy to apply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the cleanser worth the price?</td>\n",
       "      <td>The majority of users feel the cleanser offers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Does the cleanser effectively reduce acne and ...   \n",
       "1  Is the cleanser gentle, or does it cause dryne...   \n",
       "2    Does it help control oil and sebum on the skin?   \n",
       "3  How is the texture and fragrance of the cleanser?   \n",
       "4                   Is the cleanser worth the price?   \n",
       "\n",
       "                                              Answer  \n",
       "0  Most reviewers (45.08% positive) are satisfied...  \n",
       "1  The reviews describe it as gentle and non-irri...  \n",
       "2  While oil control is not heavily discussed, us...  \n",
       "3  Most reviewers find the cleanser easy to apply...  \n",
       "4  The majority of users feel the cleanser offers...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc24fa-4d08-4ad0-9baa-8ed697078821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_basics)",
   "language": "python",
   "name": "nlp_basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
